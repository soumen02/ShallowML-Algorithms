{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniProject 3: Decision Tree\n",
    "### How to run this notebook [if you need to*]\n",
    "- **Run all cells in order using _\"Run All\"_ in the Cell menu**\n",
    "- If you wish to re-run a cell, you must re-run all cells in order after restarting the kernel\n",
    "\n",
    "- In certain cases, if the output of a cell is too large, you can click the _\"Open full output data in text editor\"_ button to view it. \n",
    "\n",
    "- *If any cell returns an error, pressing restart and re-running all cells in order should fix it.*\n",
    "\n",
    "*The second dataset can take 7-10 minutes to run."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In computer science, decision tree learning uses a decision tree (as a predictive model) to go\n",
    "from observations about an item (represented in the branches) to conclusions about the item's\n",
    "target value (represented in the leaves). It is one of the predictive modeling approaches used\n",
    "in statistics, data mining and machine learning. Tree models where the target variable can\n",
    "take a discrete set of values are called classification trees; in these tree structures, leaves\n",
    "represent class labels and branches represent conjunctions of features that lead to those class\n",
    "labels. Decision trees where the target variable can take continuous values (typically real\n",
    "numbers) are called regression trees. In decision analysis, a decision tree can be used to\n",
    "visually and explicitly represent decisions and decision making. In data mining, a decision tree\n",
    "describes data (but the resulting classification tree can be an input for decision making). This\n",
    "page deals with decision trees in data mining."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Instead of growing full trees, you will use an early stopping strategy. To this end, we\n",
    "will impose a limit on the minimum number of instances at a leaf node, let this\n",
    "threshold be denoted as nmin, where nmin is described as a percentage relative to the\n",
    "size of the training dataset. For example, if the size of the training dataset is 150\n",
    "and nmin= 5, then a node will only be split further if it has more than eight instances*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive function for building the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(X, y, min_leaf=2, depth=0):\n",
    "    num_samples, num_features = X.shape\n",
    "    num_labels = len(np.unique(y))\n",
    "\n",
    "    # Base case for recursive function build_tree\n",
    "    if num_labels == 1 or num_samples < min_leaf:\n",
    "        most_common_label = np.argmax(np.bincount(y))\n",
    "        return {'feature': None, 'threshold': None, 'left': None, 'right': None, 'value': most_common_label}\n",
    "\n",
    "    # finding the best split for the current node\n",
    "    best_score = -1\n",
    "    for feat in range(num_features):\n",
    "        X_feat = X[:, feat]\n",
    "        thresholds = np.unique(X_feat)\n",
    "        for thresh in thresholds:\n",
    "            score = information_gain(X_feat, y, thresh)\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_feat = feat\n",
    "                best_thresh = thresh\n",
    "\n",
    "    # grow children recursively\n",
    "    left_idx = np.where(X[:, best_feat] <= best_thresh)[0]\n",
    "    right_idx = np.where(X[:, best_feat] > best_thresh)[0]\n",
    "    \n",
    "    left_child = build_tree(X[left_idx, :], y[left_idx], min_leaf, depth+1)\n",
    "    right_child = build_tree(X[right_idx, :], y[right_idx], min_leaf, depth+1)\n",
    "\n",
    "    return {'feature': best_feat, 'threshold': best_thresh, 'left': left_child, 'right': right_child, 'value': None}\n",
    "\n",
    "def entropy(y):\n",
    "    proportions = np.bincount(y) / len(y)\n",
    "    entropy = -np.sum([p * np.log2(p) for p in proportions if p > 0])\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def information_gain(X, y, thresh):\n",
    "    parent_loss = entropy(y)\n",
    "    left_idx = np.where(X <= thresh)[0]\n",
    "    right_idx = np.where(X > thresh)[0]\n",
    "    n, n_left, n_right = len(y), len(left_idx), len(right_idx)\n",
    "\n",
    "    if n_left == 0 or n_right == 0:\n",
    "        return 0\n",
    "\n",
    "    child_loss = (n_left / n) * entropy(y[left_idx]) + (n_right / n) * entropy(y[right_idx])\n",
    "    return parent_loss - child_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to traverse the tree recursively\n",
    "def traverse_tree(x, node):\n",
    "    if node['value'] is not None:\n",
    "        return node['value']\n",
    "\n",
    "    if x[node['feature']] <= node['threshold']:\n",
    "        return traverse_tree(x, node['left'])\n",
    "    return traverse_tree(x, node['right'])\n",
    "\n",
    "# function to make predictions on a test set\n",
    "def predict(X, tree):\n",
    "    predictions = [traverse_tree(x, tree) for x in X]\n",
    "    return np.array(predictions)\n",
    "\n",
    "# traverse the tree and print the tree\n",
    "def print_tree(node, label, depth=0):\n",
    "\n",
    "    if node['value'] is not None:\n",
    "        print('\\t' * depth, 'Predict', label[node['value']])\n",
    "        return\n",
    "\n",
    "    print('\\t' * depth, 'X[{}] <= {}'.format(node['feature'], node['threshold']))\n",
    "    print_tree(node['left'], label, depth + 1)\n",
    "    print('\\t' * depth, 'X[{}] > {}'.format(node['feature'], node['threshold']))\n",
    "    print_tree(node['right'], label, depth + 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1: Iris\n",
    "Iris: has three classes and the task is to accurately predict one of the three subtypes\n",
    "of the Iris flower given four different physical features. These features include the\n",
    "length and width of the sepals and the petals. There is a total of 150 instances with\n",
    "each class having 50 instances.\n",
    "\n",
    "*For the Iris dataset use nmin E {5, 10, 15, 20}, and calculate the accuracy using\n",
    "10 fold cross-validation for each value of min.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Average accuracy for N_min = 0.05 is 0.9400000000000001\n",
      "Standard deviation for N_min = 0.05 is 0.04975720846542675\n",
      "---------------------------------------------\n",
      "Average accuracy for N_min = 0.1 is 0.9266666666666667\n",
      "Standard deviation for N_min = 0.1 is 0.058534282980151855\n",
      "---------------------------------------------\n",
      "Average accuracy for N_min = 0.15 is 0.9266666666666667\n",
      "Standard deviation for N_min = 0.15 is 0.058534282980151855\n",
      "---------------------------------------------\n",
      "Average accuracy for N_min = 0.2 is 0.9266666666666667\n",
      "Standard deviation for N_min = 0.2 is 0.058534282980151855\n",
      "---------------------------------------------\n",
      " X[2] <= 1.9\n",
      "\t Predict Iris-setosa\n",
      " X[2] > 1.9\n",
      "\t X[3] <= 1.7\n",
      "\t\t X[2] <= 5.0\n",
      "\t\t\t X[0] <= 4.9\n",
      "\t\t\t\t Predict Iris-versicolor\n",
      "\t\t\t X[0] > 4.9\n",
      "\t\t\t\t Predict Iris-versicolor\n",
      "\t\t X[2] > 5.0\n",
      "\t\t\t Predict Iris-virginica\n",
      "\t X[3] > 1.7\n",
      "\t\t X[2] <= 4.8\n",
      "\t\t\t Predict Iris-virginica\n",
      "\t\t X[2] > 4.8\n",
      "\t\t\t Predict Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('iris.csv', header=None)\n",
    "data = df.to_numpy()\n",
    "\n",
    "# change species to numerical values\n",
    "for i in range(len(data)):\n",
    "    if data[i][4] == 'Iris-setosa':\n",
    "        data[i][4] = int(0)\n",
    "    elif data[i][4] == 'Iris-versicolor':\n",
    "        data[i][4] = int(1)\n",
    "    elif data[i][4] == 'Iris-virginica':\n",
    "        data[i][4] = int(2)\n",
    "\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    " \n",
    "# fix dtype(0) error\n",
    "X = X.astype(np.float64)\n",
    "y = y.astype(np.float64)\n",
    "label_to_int = {label: i for i, label in enumerate(np.unique(y))}\n",
    "y = np.array([label_to_int[label] for label in y])\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=8966)\n",
    "\n",
    "nmin = [0.05, 0.10, 0.15, 0.20]\n",
    "for i in nmin:\n",
    "    clf_list = []\n",
    "    avg_accuracy = []\n",
    "    std_dev = []\n",
    "    # iterate over each fold\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        iris_clf = build_tree(X_train, y_train, i*len(X_train))\n",
    "        y_pred = predict(X_test, iris_clf)\n",
    "\n",
    "        clf_list.append(iris_clf)\n",
    "        avg_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "        std_dev.append(np.std(y_pred == y_test) / np.sqrt(y_pred.shape[0]))\n",
    "    print('---------------------------------------------')\n",
    "    print('Average accuracy for N_min =', i, 'is', np.mean(avg_accuracy))\n",
    "    print('Standard deviation for N_min =', i, 'is', np.mean(std_dev))\n",
    "\n",
    "print('---------------------------------------------')\n",
    "print_tree(clf_list[0], ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2: Spambase\n",
    "\n",
    "Spambase: is a binary classification task and the objective is to classify email\n",
    "messages as being spam or not. To this end the dataset uses for seven text-based\n",
    "features to represent each email message. There are about 4600 instances.\n",
    "\n",
    "*For the Spambase dataset use nmin E {5, 10, 15, 20, 25}, and calculate the\n",
    "accuracy using 10 fold cross-validation for each value of nmin.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Average accuracy for N_min = 0.05 is 0.9015495614448741\n",
      "Standard deviation for N_min = 0.05 is 0.013755602043015975\n",
      "---------------------------------------------\n",
      "Average accuracy for N_min = 0.1 is 0.8958936150146185\n",
      "Standard deviation for N_min = 0.1 is 0.014194888988927368\n",
      "---------------------------------------------\n",
      "Average accuracy for N_min = 0.15 is 0.8765462604923135\n",
      "Standard deviation for N_min = 0.15 is 0.015281801132000988\n",
      "---------------------------------------------\n",
      "Average accuracy for N_min = 0.2 is 0.8628586249174761\n",
      "Standard deviation for N_min = 0.2 is 0.016000284751655894\n",
      "---------------------------------------------\n",
      "Average accuracy for N_min = 0.25 is 0.8300396114307272\n",
      "Standard deviation for N_min = 0.25 is 0.017433830691136826\n"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt('spambase.csv', delimiter=',')\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "label_to_int = {label: i for i, label in enumerate(np.unique(y))}\n",
    "y = np.array([label_to_int[label] for label in y])\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=101)\n",
    "\n",
    "nmin = [0.05, 0.10, 0.15, 0.20, 0.25]\n",
    "for i in nmin:\n",
    "    avg_accuracy = []\n",
    "    std_dev = []\n",
    "    spam_clf_list = []\n",
    "    # iterate over each fold\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        spam_clf = build_tree(X_train, y_train, i*len(X_train))\n",
    "        y_pred = predict(X_test, spam_clf)\n",
    "\n",
    "        spam_clf_list.append(spam_clf)\n",
    "        avg_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "        std_dev.append(np.std(y_pred == y_test) / np.sqrt(y_pred.shape[0]))\n",
    "    print('---------------------------------------------')\n",
    "    print('Average accuracy for N_min =', i, 'is', np.mean(avg_accuracy))\n",
    "    print('Standard deviation for N_min =', i, 'is', np.mean(std_dev))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__In both the datasets, we can see that the smaller the value of N_min, the better the accuracy. This is because the smaller the value of N_min, the more the number of splits and hence the more the number of nodes. This leads to a more complex tree and hence a better accuracy.__\n",
    "\n",
    "__But, this can also lead to overfitting. Hence, we need to find the optimal value of N_min which gives the best accuracy without overfitting.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
